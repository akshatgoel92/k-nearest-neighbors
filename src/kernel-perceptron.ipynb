{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do for this script:\n",
    "# Test each helper function\n",
    "# Make plots of losses to see whether it is converging\n",
    "# Check shapes of all matrices\n",
    "# Take a look at the actual data in each matrix\n",
    "# Take a look at each weight\n",
    "# Take a look at whether kernel arguments are being sent in correctly\n",
    "# Test Gaussian kernel\n",
    "# Time run\n",
    "# Implement k-fold cross validation\n",
    "# Implement 'many runs' mode to do 20 runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questions: \n",
    "# Do we shuffle the data at each epoch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Potential report content\n",
    "# Talk about effect of dimensionality on overfitting\n",
    "# Expand the Gaussian kernel into its feature map and speak about the role of c as a regularizer\n",
    "# Try to answer the question: for what values of C does Gaussian kernel mimic a polynomial kernel? \n",
    "# Potentially make a plot for the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist, pdist, squareform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, name):\n",
    "    '''\n",
    "    Takes in a folder path\n",
    "    and dataset name and loads\n",
    "    the corresponding dataset\n",
    "    divided into features X\n",
    "    and labels Y as numpy arrays.\n",
    "    '''\n",
    "    data = np.loadtxt(os.path.join(path, name))\n",
    "    X, Y = data[:, 1:], data[:, 0]\n",
    "\n",
    "    return(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(X, Y):\n",
    "    '''\n",
    "    Takes in datasets X and Y\n",
    "    and returns a random permutation\n",
    "    of these datasets.\n",
    "    '''\n",
    "    perm = np.random.permutation(max(Y.shape))\n",
    "    X = X[perm, :]\n",
    "    Y = Y[perm]\n",
    "\n",
    "    return(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(arr, shape=(16, 16)):\n",
    "    '''\n",
    "    Takes in a numpy array\n",
    "    of pixel values and an image shape \n",
    "    and displays the associated image.\n",
    "    '''\n",
    "    img = arr.reshape(shape)\n",
    "    imgplot = plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, Y, train_percent):\n",
    "    '''\n",
    "    Take datasets X and Y and split them \n",
    "    into train_percent*100 % training dataset\n",
    "    and (1 - train_percent)*100 % hold-out dataset.\n",
    "    '''\n",
    "    # Calculate no. of training examples based on user specified percentage\n",
    "    # Here we use 2/3, 1/3 by default as required by the assignment\n",
    "    n_train = round(train_percent*max(Y.shape))\n",
    "    \n",
    "    # Filter the dataframe to get training and testing rows\n",
    "    X_train = X[:n_train, :]\n",
    "    Y_train = Y[:n_train]\n",
    "    \n",
    "    # Validation set\n",
    "    X_val = X[n_train:, :]\n",
    "    Y_val = Y[n_train:]\n",
    "    \n",
    "    # Return statement\n",
    "    return(X_train, X_val, Y_train, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polynomial_kernel(X, X_, d):\n",
    "    '''\n",
    "    Take in two matrices X and X_ and\n",
    "    a kernel parameter d and return the\n",
    "    Gram Matrix K(x, x_) for polynomial \n",
    "    kernel with parameter d. Note that \n",
    "    this will return polynomials of exactly\n",
    "    degree d like the assignment says and \n",
    "    not a sum of polynomials upto degree d.\n",
    "    '''\n",
    "    return(np.power(np.dot(X, X_.T), d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gaussian_kernel(X, X_, c):\n",
    "    '''\n",
    "    Take in two matrices and a kernel\n",
    "    parameter C and return the Gram matrix\n",
    "    K(x, x_) for the Gaussian kernel between\n",
    "    X and X_.\n",
    "    '''\n",
    "    # Compute pairwise distances\n",
    "    K = np.exp(c*(np.einsum('ij,ij->i',X, X)[:,None] + np.einsum('ij,ij->i',X_,X_) - 2*np.dot(X,X_.T)))\n",
    "    \n",
    "    # Return statement\n",
    "    return(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_folds(X, Y, k):\n",
    "    '''\n",
    "    Take in two arrays for features\n",
    "    and corresponding labels respectively\n",
    "    as well as a user-specified number of\n",
    "    folds. Return the X and Y arrays divided\n",
    "    into the k sub-arrays where each sub-array\n",
    "    is a fold.\n",
    "    '''\n",
    "    fold_length = int(np.round(max(X_train.shape)/k))\n",
    "    X_train_folds = np.split(X_train_folds.copy(), fold_length)\n",
    "    Y_train_folds = np.split(Y_train_folds.copy(), fold_length)\n",
    "    \n",
    "    return(X_train_folds, Y_train_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(alpha, K_examples):\n",
    "    '''\n",
    "    Returns raw predictions and class predictions\n",
    "    given alpha weights and Gram matrix K_examples.\n",
    "    '''\n",
    "    # Take the maximum argument in each column\n",
    "    Y_hat = alpha @ K_examples\n",
    "    preds = np.argmax(Y_hat, axis = 0)\n",
    "    \n",
    "    # Return statement\n",
    "    return(Y_hat, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(target, pred):\n",
    "    '''\n",
    "    Returns binary accuracy given \n",
    "    two arrays containing true values\n",
    "    and predicted values respectively.\n",
    "    '''\n",
    "    return np.sum(target==pred)/max(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_matrix(target, pred):\n",
    "    '''\n",
    "    Returns a confusion matrix given two\n",
    "    arrays containing the true values 'target'\n",
    "    and predicted values 'pred' respectively.\n",
    "    Interpretation: we put target values in the \n",
    "    rows and predicted values in the columns. So \n",
    "    we have that for example the element (2, 1) \n",
    "    contains all the elements which have true labels\n",
    "    2 but are classified as 1.\n",
    "    '''\n",
    "    # The confusion matrix should be a square matrix with\n",
    "    # no. of rows and columns equal to the number of unique\n",
    "    # values in the target: this is what we compute below\n",
    "\n",
    "    # Compute the no. of unique values in target\n",
    "    cf_dim = len(np.unique(target))\n",
    "    \n",
    "    # Initialize the confusion matrix as zeros\n",
    "    cf= np.zeros((cf_dim, cf_dim))\n",
    "\n",
    "    # Now go through each target value\n",
    "    # Look at the corresponding prediction\n",
    "    # Update the corresponding cell in the confusion matrix\n",
    "    for i in range(len(target)):\n",
    "        cf[int(target[i]) - 1, int(pred[i]) - 1] += 1\n",
    "    \n",
    "    # Return statement\n",
    "    return(cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(history):\n",
    "    '''\n",
    "    This function takes in a \n",
    "    dictionary containing an\n",
    "    epoch-wise record of training\n",
    "    and validation set binary accuracies\n",
    "    and returns the epoch at which the highest\n",
    "    binary accuracy was reached on the dev. set \n",
    "    along with the associated accuracies on both\n",
    "    training and dev. set at that epoch.\n",
    "    '''\n",
    "    # Store results\n",
    "    best_epoch = np.array(history[\"val_accuracies\"]).argmax()\n",
    "    best_training_accuracy = history['accuracies'][best_epoch]\n",
    "    best_dev_accuracy = history['val_accuracies'][best_epoch]\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"best training accuracy: {history['accuracies'][best_epoch]}\")\n",
    "    print(f\"best dev accuracy: {history['dev_accuracies'][best_epoch]}\")\n",
    "    print(f\"best epoch: {best_epoch}\")\n",
    "    \n",
    "    return(best_epoch, best_training_accuracy, best_dev_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_kernel_perceptron(X_train, Y_train, X_val, Y_val, epochs, lr, kernel_type, kernel_args, n_classes):\n",
    "    '''\n",
    "    This is the main training loop for\n",
    "    the kernel perceptron algorithm.\n",
    "    '''\n",
    "    # Store a record of training and validation accuracies at each epoch\n",
    "    history = {\n",
    "        \"train_accuracies\": [],\n",
    "        \"val_accuracies\": []\n",
    "    }\n",
    "    \n",
    "    # Transform X according to the user specified kernel\n",
    "    # Can be either polynomial kernel or Gaussian kernel\n",
    "    # Do this for both training and validation set\n",
    "    if kernel_type == 'polynomial':\n",
    "        K_train = get_polynomial_kernel(X_train, X_train, **kernel_args)\n",
    "        K_val = get_polynomial_kernel(X_train, X_val, **kernel_args)\n",
    "    \n",
    "    elif kernel_type == 'gaussian':\n",
    "        K_train = get_gaussian_kernel(X_train, X_train, **kernel_args)\n",
    "        K_val = get_gaussian_kernel(X_train, X_val, **kernel_args)\n",
    "    \n",
    "    # Initialize alpha weights and store \n",
    "    # the number of samples\n",
    "    alpha = np.zeros((n_classes, K_train.shape[0]))\n",
    "    n_samples = max(Y_train.shape)\n",
    "    \n",
    "    # Run for a fixed user-specified number of epochs\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Print the epoch number to track progress\n",
    "        print('This is epoch number: {}'.format(epoch))\n",
    "\n",
    "        # Do this for each example in the dataset\n",
    "        for i in range(n_samples):\n",
    "\n",
    "            # Compute the prediction with the current weights:\n",
    "            # dim(A) --> (10, 6199), dim(X_train[i, :]) ---> (6199, 1) ====> dim(y_hat) --> 10 X 1\n",
    "            Y_hat, _ = get_predictions(alpha, K_train[i, :])\n",
    "            \n",
    "            # Now first make a matrix Y with dim(Y) ---> (n_classes,) which is only filled with -1\n",
    "            # Then get the label from the Y_train matrix\n",
    "            # If this label is 6 then we want to change the 6th index to 1\n",
    "            Y = np.full(n_classes, -1)\n",
    "            Y[int(Y_train[i])] = 1\n",
    "            \n",
    "            # Compute sign of predictions\n",
    "            # This is used in the update\n",
    "            signs = np.ones(Y_hat.shape)\n",
    "            signs[Y_hat <= 0] = -1\n",
    "            \n",
    "            # Check if the prediction is correct against the labels\n",
    "            # If it is correct we don't need to make any updates: we just move to the next iteration\n",
    "            # If it is not correct then we update the weights and biases in the direction of the label\n",
    "            alpha[Y*Y_hat <= 0, i] -= (signs[Y*Y_hat <= 0]) \n",
    "            \n",
    "        # We finally compute predictions and accuracy at the end of each epoch\n",
    "        # It is a mistake if the class with the highest predicted value does not equal the true label\n",
    "        # mistakes += int((np.argmax(Y_hat) + 1) != int(Y_train[i]))\n",
    "        Y_hat_train, preds_train = get_predictions(alpha, K_train)\n",
    "        train_accuracy = get_accuracy(Y_train, preds_train)\n",
    "            \n",
    "        # Now we compute validation predictions\n",
    "        Y_hat_val, preds_val = get_predictions(alpha, K_val)\n",
    "        val_accuracy = get_accuracy(Y_val, preds_val)\n",
    "        \n",
    "        # We append to the history dictionary as a record\n",
    "        history['train_accuracies'].append(train_accuracy)\n",
    "        history['val_accuracies'].append(val_accuracy)\n",
    "        \n",
    "        # We print the accuracies at the end of each epoch\n",
    "        msg = 'The number of {} accuracy on epoch {} is {}...'\n",
    "        print(msg.format('train', epoch, train_accuracy))\n",
    "        print(msg.format('validation', epoch, val_accuracy))\n",
    "    \n",
    "    # At the end of the entire run we get confusion matrices\n",
    "    train_cf = get_confusion_matrix(Y_train, preds_train)\n",
    "    val_cf = get_confusion_matrix(Y_val, preds_val)\n",
    "        \n",
    "    # Return statement\n",
    "    return(train_cf, val_cf, preds_val, preds_train, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_perceptron_training(epochs, lr, data_path = os.path.join('..', 'data'), name = 'zipcombo.dat', \n",
    "                            kernel_type = 'polynomial', d = 5, n_classes=10):\n",
    "    '''\n",
    "    Execute the training steps above and generate\n",
    "    the results that have been specified in the report.\n",
    "    '''\n",
    "    # Prepare data for the perceptron\n",
    "    X, Y = load_data(data_path, name)\n",
    "    \n",
    "    # Shuffle the dataset before splitting it\n",
    "    X, Y = shuffle_data(X, Y)\n",
    "    \n",
    "    # Split the data into training and validation set \n",
    "    X_train, X_val, Y_train, Y_val = split_data(X, Y, 0.66666)\n",
    "    \n",
    "    # Construct kernel arguments dictionary\n",
    "    if kernel_type == 'polynomial': kernel_args = {'d': d}\n",
    "\n",
    "    # Call the perceptron training with the given epochs\n",
    "    results = train_kernel_perceptron(X_train, Y_train, \n",
    "                                      X_val, Y_val, epochs, lr, \n",
    "                                      kernel_type, kernel_args, n_classes)\n",
    "    \n",
    "    # Return statement\n",
    "    return(results, Y_train, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_k_fold_cross_val(epochs, lr, data_path = os.path.join('..', 'data'), \n",
    "                         name = 'zipcombo.dat', kernel_type = 'polynomial', \n",
    "                         d = 5, n_classes=10, k = 5):\n",
    "    '''\n",
    "    Execute the training steps above and generate\n",
    "    the results that have been specified in the report.\n",
    "    '''\n",
    "    # Prepare data for the perceptron\n",
    "    X, Y = load_data(data_path, name)\n",
    "    \n",
    "    # Shuffle the dataset before splitting it\n",
    "    X, Y = shuffle_data(X, Y)\n",
    "    \n",
    "    # Construct kernel arguments dictionary\n",
    "    if kernel_type == 'polynomial': kernel_args = {'d': d}\n",
    "    \n",
    "    # Split the data into training and validation set \n",
    "    X_folds, Y_folds = get_k_folds(X, Y, 5)\n",
    "    \n",
    "    # Now go through each fold\n",
    "    # Every fold becomes the hold-out set at least once\n",
    "    for fold_no, X_fold in enumerate(X_folds):\n",
    "        \n",
    "        # Put in the x-values\n",
    "        X_train = X_folds[:fold_no] + X_folds[fold_no+1:]\n",
    "        X_val = X_folds[fold_no]\n",
    "        \n",
    "        # Put in the Y values\n",
    "        Y_train = Y_folds[:fold_no] + Y_folds[fold_no+1:]\n",
    "        Y_val =  Y_val[:fold_no] + Y_val[fold_no+1:]\n",
    "        \n",
    "        # Call the perceptron training with the given epochs\n",
    "        history = train_kernel_perceptron(X_train, Y_train, \n",
    "                                          X_val, Y_val, epochs, lr, \n",
    "                                          kernel_type, kernel_args, n_classes)\n",
    "    \n",
    "    # Return statement\n",
    "    return(results, Y_train, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is epoch number: 0\n",
      "The number of train accuracy on epoch 0 is 0.9890304887885143...\n",
      "The number of validation accuracy on epoch 0 is 0.9593417231364957...\n",
      "This is epoch number: 1\n",
      "The number of train accuracy on epoch 1 is 0.9953218261009841...\n",
      "The number of validation accuracy on epoch 1 is 0.962568570506615...\n",
      "This is epoch number: 2\n",
      "The number of train accuracy on epoch 2 is 0.998709469269237...\n",
      "The number of validation accuracy on epoch 2 is 0.9693449499838658...\n",
      "This is epoch number: 3\n",
      "The number of train accuracy on epoch 3 is 0.9995160509759639...\n",
      "The number of validation accuracy on epoch 3 is 0.9693449499838658...\n",
      "This is epoch number: 4\n",
      "The number of train accuracy on epoch 4 is 0.9996773673173093...\n",
      "The number of validation accuracy on epoch 4 is 0.9706356889319135...\n",
      "This is epoch number: 5\n",
      "The number of train accuracy on epoch 5 is 0.9995160509759639...\n",
      "The number of validation accuracy on epoch 5 is 0.9722491126169732...\n",
      "This is epoch number: 6\n",
      "The number of train accuracy on epoch 6 is 0.9996773673173093...\n",
      "The number of validation accuracy on epoch 6 is 0.9683768957728299...\n",
      "This is epoch number: 7\n",
      "The number of train accuracy on epoch 7 is 0.9995160509759639...\n",
      "The number of validation accuracy on epoch 7 is 0.9696676347208777...\n",
      "This is epoch number: 8\n",
      "The number of train accuracy on epoch 8 is 0.9996773673173093...\n",
      "The number of validation accuracy on epoch 8 is 0.9703130041949016...\n",
      "This is epoch number: 9\n",
      "The number of train accuracy on epoch 9 is 0.9996773673173093...\n",
      "The number of validation accuracy on epoch 9 is 0.9699903194578896...\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed for random number generator to ensure reproducibility\n",
    "np.random.seed(13290138)\n",
    "\n",
    "# Call training function once\n",
    "results, Y_train, Y_val = run_kernel_perceptron_training(epochs=10, lr=1)\n",
    "train_cf, val_cf, preds_val, preds_train, history = results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
