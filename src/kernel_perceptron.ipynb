{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(alpha, K_examples):\n",
    "    '''\n",
    "    Returns raw predictions and class predictions\n",
    "    given alpha weights and Gram matrix K_examples.\n",
    "    '''\n",
    "    # Take the maximum argument in each column\n",
    "    Y_hat = alpha @ K_examples\n",
    "    preds = np.argmax(Y_hat, axis = 0)\n",
    "    \n",
    "    # Return statement\n",
    "    return(Y_hat, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_update(Y_train, Y_hat, alpha, n_classes, i):\n",
    "    '''\n",
    "    Returns raw predictions and class predictions\n",
    "    given alpha weights and Gram matrix K_examples.\n",
    "    '''\n",
    "    # Now first make a matrix Y with dim(Y) ---> (n_classes,) which is only filled with -1\n",
    "    # Then get the label from the Y_train matrix\n",
    "    # If this label is 6 then we want to change the 6th index to 1\n",
    "    Y = np.full(n_classes, -1)\n",
    "    Y[int(Y_train[i])] = 1\n",
    "            \n",
    "    # Compute sign of predictions\n",
    "    # This is used in the update\n",
    "    signs = np.ones(Y_hat.shape)\n",
    "    signs[Y_hat <= 0] = -1\n",
    "            \n",
    "    # Check if the prediction is correct against the labels\n",
    "    # If it is correct we don't need to make any updates: we just move to the next iteration\n",
    "    # If it is not correct then we update the weights and biases in the direction of the label\n",
    "    alpha[Y*Y_hat <= 0, i] -= (signs[Y*Y_hat <= 0])\n",
    "    \n",
    "    return(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_kernel_perceptron(X_train, Y_train, X_val, Y_val, epochs, kernel_type, d, n_classes):\n",
    "    '''\n",
    "    This is the main training loop for\n",
    "    the kernel perceptron algorithm.\n",
    "    '''\n",
    "    # Store a record of training and validation accuracies and other data from each epoch\n",
    "    history = {\n",
    "        \"train_accuracies\": [],\n",
    "        \"val_accuracies\": [],\n",
    "        \"train_cf\": [],\n",
    "        \"val_cf\": []\n",
    "    }\n",
    "    \n",
    "    # Transform X according to the user specified kernel\n",
    "    # Can be either polynomial kernel or Gaussian kernel\n",
    "    # Do this for both training and validation set\n",
    "    if kernel_type == 'polynomial':\n",
    "        K_train = helpers.get_polynomial_kernel(X_train, X_train, d)\n",
    "        K_val = helpers.get_polynomial_kernel(X_train, X_val, d)\n",
    "    \n",
    "    elif kernel_type == 'gaussian':\n",
    "        K_train = helpers.get_gaussian_kernel(X_train, X_train, d)\n",
    "        K_val = helpers.get_gaussian_kernel(X_train, X_val, d)\n",
    "    \n",
    "    # Initialize alpha weights and store \n",
    "    # the number of samples\n",
    "    alpha = np.zeros((n_classes, K_train.shape[0]))\n",
    "    n_samples = max(Y_train.shape)\n",
    "    \n",
    "    # Run for a fixed user-specified number of epochs\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Print the epoch number to track progress\n",
    "        print('This is epoch number: {}'.format(epoch))\n",
    "\n",
    "        # Do this for each example in the dataset\n",
    "        for i in range(n_samples):\n",
    "            # Compute the prediction with the current weights:\n",
    "            # dim(A) --> (10, 6199), dim(X_train[i, :]) ---> (6199, 1) ====> dim(y_hat) --> 10 X 1\n",
    "            Y_hat, _ = get_predictions(alpha, K_train[i, :])\n",
    "            \n",
    "            # Perform update by calling the function above\n",
    "            alpha = get_update(Y_train, Y_hat, alpha, n_classes, i)\n",
    "            \n",
    "        # We finally compute predictions and accuracy at the end of each epoch\n",
    "        # It is a mistake if the class with the highest predicted value does not equal the true label\n",
    "        # mistakes += int((np.argmax(Y_hat) + 1) != int(Y_train[i]))\n",
    "        Y_hat_train, preds_train = get_predictions(alpha, K_train)\n",
    "        train_accuracy = helpers.get_accuracy(Y_train, preds_train)\n",
    "            \n",
    "        # Now we compute validation predictions\n",
    "        Y_hat_val, preds_val = get_predictions(alpha, K_val)\n",
    "        val_accuracy = helpers.get_accuracy(Y_val, preds_val)\n",
    "        \n",
    "        # At the end of each epoch we get confusion matrices\n",
    "        train_cf = helpers.get_confusion_matrix(Y_train, preds_train)\n",
    "        val_cf = helpers.get_confusion_matrix(Y_val, preds_val)\n",
    "        \n",
    "        # We append to the history dictionary as a record\n",
    "        history['train_accuracies'].append(train_accuracy)\n",
    "        history['val_accuracies'].append(val_accuracy)\n",
    "        history['train_cf'].append(train_cf)\n",
    "        history['val_cf'].append(val_cf)\n",
    "        \n",
    "        # We print the accuracies at the end of each epoch\n",
    "        msg = '{} accuracy on epoch {}: {}'\n",
    "        print(msg.format('train', epoch, train_accuracy))\n",
    "        print(msg.format('validation', epoch, val_accuracy))\n",
    "    \n",
    "    # Return statement\n",
    "    return(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_perceptron_training(epochs, data_path = os.path.join('..', 'data'), name = 'zipcombo.dat', \n",
    "                            kernel_type = 'polynomial', d = 5, n_classes=10, train_percent=0.8):\n",
    "    '''\n",
    "    Execute the training steps above and generate\n",
    "    the results that have been specified in the report.\n",
    "    '''\n",
    "    # Prepare data for the perceptron\n",
    "    X, Y = helpers.load_data(data_path, name)\n",
    "    \n",
    "    # Shuffle the dataset before splitting it\n",
    "    X, Y = helpers.shuffle_data(X, Y)\n",
    "    \n",
    "    # Split the data into training and validation set \n",
    "    X_train, X_val, Y_train, Y_val = helpers.split_data(X, Y, train_percent)\n",
    "\n",
    "    # Call the perceptron training with the given epochs\n",
    "    history = train_kernel_perceptron(X_train, Y_train, \n",
    "                                      X_val, Y_val, epochs,\n",
    "                                      kernel_type, d, n_classes)\n",
    "    \n",
    "    # Return best epoch according to dev. accuracy and the associated accuracies on both datasets\n",
    "    best_epoch, best_training_accuracy, best_dev_accuracy = helpers.get_best_results(history)\n",
    "    \n",
    "    # Return statement\n",
    "    return(history, best_epoch, best_training_accuracy, best_dev_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_k_fold_cross_val(epochs, data_path = os.path.join('..', 'data'), \n",
    "                         name = 'zipcombo.dat', kernel_type = 'polynomial', \n",
    "                         d = 5, n_classes=10, k = 5):\n",
    "    '''\n",
    "    Execute the training steps above and generate\n",
    "    the results that have been specified in the report.\n",
    "    '''\n",
    "    # Prepare data for the perceptron\n",
    "    X, Y = helpers.load_data(data_path, name)\n",
    "    \n",
    "    # Shuffle the dataset before splitting it\n",
    "    X, Y = helpers.shuffle_data(X, Y)\n",
    "    \n",
    "    # Split the data into training and validation set \n",
    "    X_folds, Y_folds = helpers.get_k_folds(X, Y, k)\n",
    "    \n",
    "    # Initiate histories object\n",
    "    histories = []\n",
    "    \n",
    "    # Now go through each fold : every fold becomes the hold-out set at least once\n",
    "    for fold_no in range(k):\n",
    "        \n",
    "        # Put in the x-values\n",
    "        X_train = np.concatenate(X_folds[:fold_no] + X_folds[fold_no+1:])\n",
    "        X_val = X_folds[fold_no]\n",
    "        \n",
    "        # Put in the Y values\n",
    "        Y_train = np.concatenate(Y_folds[:fold_no] + Y_folds[fold_no+1:])\n",
    "        Y_val =  Y_folds[fold_no]\n",
    "        \n",
    "        # Call the perceptron training with the given epochs\n",
    "        history = train_kernel_perceptron(X_train, Y_train, \n",
    "                                          X_val, Y_val, epochs,\n",
    "                                          kernel_type, d, n_classes)\n",
    "        \n",
    "        # Append to the histories file the epoch by epoch record of each fold\n",
    "        histories.append(history)\n",
    "    \n",
    "    # Get avg. accuracies by epoch across folds\n",
    "    avg_history = helpers.get_avg_results(histories)\n",
    "    \n",
    "    # Return best epoch according to dev. accuracy and the associated accuracies on both datasets\n",
    "    best_epoch, best_training_accuracy, best_dev_accuracy = helpers.get_best_results(avg_history)\n",
    "        \n",
    "    # Return statement\n",
    "    return(avg_history, best_epoch, best_training_accuracy, best_dev_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multiple(params, kwargs):\n",
    "    \n",
    "    histories = {\n",
    "        \n",
    "        'params': params, \n",
    "        'history': [],\n",
    "        'best_epoch': [],\n",
    "        'best_training_accuracy': [],\n",
    "        'best_dev_accuracy': [],\n",
    "    }\n",
    "    \n",
    "    for param in params:\n",
    "        history, best_epoch, best_training_accuracy, best_dev_accuracy = run_perceptron_training(**kwargs, d=param)\n",
    "        histories['history'].append(history)\n",
    "        histories['best_epoch'].append(best_epoch)\n",
    "        histories['best_training_accuracy'].append(best_training_accuracy)\n",
    "        histories['best_dev_accuracy'].append(best_dev_accuracy)\n",
    "    \n",
    "    return(histories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grid_search(params, kwargs):\n",
    "    \n",
    "    \n",
    "    histories = {\n",
    "        \n",
    "            'params': params, \n",
    "            'best_epoch': [],\n",
    "            'best_training_accuracy': [],\n",
    "            'best_dev_accuracy': [],\n",
    "    }\n",
    "    \n",
    "    for param in params: \n",
    "        _, best_epoch, best_training_accuracy, best_dev_accuracy = run_k_fold_cross_val(d=param, **kwargs)\n",
    "        histories['best_epoch'].append(best_epoch)\n",
    "        histories['best_training_accuracy'].append(best_training_accuracy)\n",
    "        histories['best_dev_accuracy'].append(best_dev_accuracy)\n",
    "    \n",
    "    \n",
    "    return(histories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def main(params, multiple_run_args, grid_search_args):\n",
    "\n",
    "    # Call training function multiple runs\n",
    "    multiple_histories = run_multiple(params, multiple_run_args)\n",
    "    \n",
    "    # Call training function with k-fold cross validation\n",
    "    cross_val_histories = run_grid_search(params, grid_search_args)\n",
    "    \n",
    "    # Return statement\n",
    "    retrun(multiple_histories, cross_val_histories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "np.random.seed(13290138)\n",
    "\n",
    "params = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "multiple_run_args = {\n",
    "    \n",
    "    'epochs': 100, \n",
    "    'data_path': os.path.join('..', 'data'), \n",
    "    'name': 'zipcombo.dat', \n",
    "    'kernel_type': 'polynomial', \n",
    "    'n_classes': 10,\n",
    "    'train_percent': 0.8   \n",
    "}\n",
    "\n",
    "\n",
    "grid_search_args = {\n",
    "    \n",
    "    'epochs': 100, \n",
    "    'data_path': os.path.join('..', 'data'), \n",
    "    'name': 'zipcombo.dat', \n",
    "    'kernel_type': 'polynomial', \n",
    "    'n_classes': 10,\n",
    "    'k': 5\n",
    "    \n",
    "}\n",
    "\n",
    "multiple_histories, cross_val_histories = main(params, grid_search_args, multiple_run_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do for this script:\n",
    "# Make plots of losses to see whether it is converging\n",
    "# Check shapes of all matrices\n",
    "# Take a look at the actual data in each matrix\n",
    "# Take a look at each weight\n",
    "# Take a look at whether kernel arguments are being sent in correctly\n",
    "# Test Gaussian kernel\n",
    "\n",
    "# Potential report content\n",
    "# Talk about effect of dimensionality on overfitting\n",
    "# Expand the Gaussian kernel into its feature map and speak about the role of c as a regularizer\n",
    "# Try to answer the question: for what values of C does Gaussian kernel mimic a polynomial kernel? \n",
    "# Potentially make a plot for the above\n",
    "\n",
    "# Questions: \n",
    "# Do we shuffle the data at each epoch?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
